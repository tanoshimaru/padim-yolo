#!/usr/bin/env python3
"""
PaDiM ç•°å¸¸æ¤œçŸ¥ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

å­¦ç¿’ãƒ‡ãƒ¼ã‚¿:
- images/grid_XX (00-15): æ­£å¸¸ç”»åƒ (äººãŒå†™ã£ã¦ã„ã‚‹æ­£å¸¸ãªã‚°ãƒªãƒƒãƒ‰åˆ¥ç”»åƒ)
- images/no_person: æ­£å¸¸ç”»åƒ (äººãŒå†™ã£ã¦ã„ãªã„ç”»åƒ)

æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿:
- images/test/normal: æ­£å¸¸ãªæ¤œè¨¼ç”»åƒ
- images/test/anomaly: ç•°å¸¸ãªæ¤œè¨¼ç”»åƒ

å­¦ç¿’ã¯æ­£å¸¸ç”»åƒã®ã¿ã§è¡Œã„ã€ç•°å¸¸æ¤œçŸ¥ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚
testãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®normal/anomalyã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¯æ¨è«–è©•ä¾¡æ™‚ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚
"""

import sys
import logging
from datetime import datetime
from pathlib import Path
from typing import List
import argparse

import lightning.pytorch as pl
from anomalib.models import Padim
from anomalib.data import Folder
import shutil
import torch


def create_unified_training_dir(
    images_dir: str,
    training_dir: str = "temp_training_data",
    image_size: tuple = (224, 224),
) -> tuple:
    """å…¨ã¦ã®æ­£å¸¸ç”»åƒã‚’çµ±åˆã—ãŸä¸€æ™‚çš„ãªå­¦ç¿’ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆï¼ˆã‚·ã‚§ãƒ«ã‚¹ã‚¯ãƒªãƒ—ãƒˆä½¿ç”¨ï¼‰"""
    import subprocess

    logger = logging.getLogger(__name__)

    training_path = Path(training_dir)
    normal_dir = training_path / "normal"

    # ãƒªã‚µã‚¤ã‚ºå‡¦ç†ãŒå¿…è¦ãªãŸã‚ã€æ—¢å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å¼·åˆ¶çš„ã«å†ä½œæˆ
    if normal_dir.exists():
        logger.info("ãƒªã‚µã‚¤ã‚ºå‡¦ç†ã®ãŸã‚æ—¢å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‰Šé™¤ã—ã¦å†ä½œæˆã—ã¾ã™")
        for file in normal_dir.glob("*"):
            if file.is_file():
                file.unlink()

    # ã‚·ã‚§ãƒ«ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§é«˜é€Ÿãƒªã‚µã‚¤ã‚º&ã‚³ãƒ”ãƒ¼ã‚’å®Ÿè¡Œ
    script_path = Path(__file__).parent / "copy_training_images_resized.sh"
    target_size = f"{image_size[0]}x{image_size[1]}"  # 224x224å½¢å¼
    try:
        result = subprocess.run(
            [str(script_path), images_dir, training_dir, target_size],
            capture_output=True,
            text=True,
            check=True,
        )

        # æœ€å¾Œã®è¡Œã‹ã‚‰ç”»åƒæ•°ã‚’å–å¾—
        output_lines = result.stdout.strip().split("\n")
        for line in output_lines[:-1]:  # æœ€å¾Œã®è¡Œä»¥å¤–ã‚’å‡ºåŠ›
            logger.info(line)

        total_images = int(output_lines[-1]) if output_lines else 0

    except subprocess.CalledProcessError as e:
        logger.error(f"ã‚·ã‚§ãƒ«ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e.stderr}")
        return str(training_path), str(normal_dir), 0
    except (ValueError, IndexError):
        logger.error("ã‚·ã‚§ãƒ«ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‹ã‚‰ã®å‡ºåŠ›è§£æã‚¨ãƒ©ãƒ¼")
        return str(training_path), str(normal_dir), 0

    logger.info(f"çµ±åˆã•ã‚ŒãŸå­¦ç¿’ç”¨æ­£å¸¸ç”»åƒ: {total_images} æš")
    return str(training_path), str(normal_dir), total_images


def cleanup_training_dir(training_dir: str):
    """å­¦ç¿’ç”¨ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‰Šé™¤"""
    logger = logging.getLogger(__name__)

    training_path = Path(training_dir)
    if training_path.exists():
        shutil.rmtree(training_path)
        logger.info(f"ä¸€æ™‚å­¦ç¿’ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‰Šé™¤ã—ã¾ã—ãŸ: {training_dir}")


def setup_logging() -> logging.Logger:
    """ãƒ­ã‚°è¨­å®š"""
    log_dir = Path("logs")
    log_dir.mkdir(exist_ok=True)

    log_filename = f"train_padim_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s",
        handlers=[
            logging.FileHandler(log_dir / log_filename, encoding="utf-8"),
            logging.StreamHandler(sys.stdout),
        ],
    )
    return logging.getLogger(__name__)


def check_data_structure(images_dir: str) -> dict:
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹é€ ã®ç¢ºèª"""
    images_path = Path(images_dir)

    # ã‚°ãƒªãƒƒãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºèª
    grid_dirs = []
    for i in range(16):  # grid_00 ã‹ã‚‰ grid_15
        grid_dir = images_path / f"grid_{i:02d}"
        if grid_dir.exists() and grid_dir.is_dir():
            grid_dirs.append(grid_dir)

    # no_personãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºèª
    no_person_dir = images_path / "no_person"

    # testãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºèªï¼ˆnormal/anomalyã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå«ã‚€ï¼‰
    test_dir = images_path / "test"
    test_normal_dir = test_dir / "normal" if test_dir.exists() else None
    test_anomaly_dir = test_dir / "anomaly" if test_dir.exists() else None

    return {
        "grid_dirs": grid_dirs,
        "no_person_dir": no_person_dir if no_person_dir.exists() else None,
        "test_dir": test_dir if test_dir.exists() else None,
        "test_normal_dir": test_normal_dir
        if test_normal_dir and test_normal_dir.exists()
        else None,
        "test_anomaly_dir": test_anomaly_dir
        if test_anomaly_dir and test_anomaly_dir.exists()
        else None,
    }


def count_images_in_directory(directory: Path) -> int:
    """ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
    if not directory.exists():
        return 0

    image_extensions = {".jpg", ".jpeg", ".png", ".bmp", ".tiff", ".tif"}
    count = 0

    for file_path in directory.iterdir():
        if file_path.is_file() and file_path.suffix.lower() in image_extensions:
            count += 1

    return count


def get_training_info(images_dir: str) -> tuple:
    """å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã®æƒ…å ±ã‚’å–å¾—"""
    logger = logging.getLogger(__name__)

    data_structure = check_data_structure(images_dir)

    # æ­£å¸¸ç”»åƒæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
    total_normal_images = 0

    # grid_XX ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰æ­£å¸¸ç”»åƒã‚’ã‚«ã‚¦ãƒ³ãƒˆ
    for grid_dir in data_structure["grid_dirs"]:
        image_count = count_images_in_directory(grid_dir)
        logger.info(f"{grid_dir.name}: {image_count} ç”»åƒ")
        total_normal_images += image_count

    # no_person ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰æ­£å¸¸ç”»åƒã‚’ã‚«ã‚¦ãƒ³ãƒˆ
    if data_structure["no_person_dir"]:
        no_person_count = count_images_in_directory(data_structure["no_person_dir"])
        logger.info(f"no_person: {no_person_count} ç”»åƒ")
        total_normal_images += no_person_count

    # test ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç”»åƒæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
    test_images = 0
    if data_structure["test_dir"]:
        test_images = count_images_in_directory(data_structure["test_dir"])
        logger.info(f"test: {test_images} ç”»åƒ")

    logger.info(f"å­¦ç¿’ç”¨æ­£å¸¸ç”»åƒ: {total_normal_images} æš")
    logger.info(f"æ¤œè¨¼ç”¨ç”»åƒ: {test_images} æš")

    # å®Ÿéš›ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹ã‚’è¿”ã™
    normal_dirs = [str(d) for d in data_structure["grid_dirs"]]
    if data_structure["no_person_dir"]:
        normal_dirs.append(str(data_structure["no_person_dir"]))

    test_dir = str(data_structure["test_dir"]) if data_structure["test_dir"] else None

    return normal_dirs, test_dir, total_normal_images, test_images


def create_padim_model(
    image_size: tuple = (224, 224),  # ResNetæ¨™æº–ã‚µã‚¤ã‚ºï¼ˆæœ€é©ãªå‡¦ç†åŠ¹ç‡ï¼‰
    backbone: str = "resnet18",
    layers: List[str] | None = None,
) -> Padim:
    """PaDiMãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ"""
    if layers is None:
        layers = ["layer1", "layer2", "layer3"]

    pre_processor = Padim.configure_pre_processor(image_size=image_size)

    model = Padim(
        backbone=backbone,
        layers=layers,
        pre_trained=True,
        pre_processor=pre_processor,
    )

    return model


def train_padim_model(
    images_dir: str,
    model_save_path: str = "models/padim_model.ckpt",
    image_size: tuple = (224, 224),  # ResNetæ¨™æº–ã‚µã‚¤ã‚ºï¼ˆæœ€é©ãªå‡¦ç†åŠ¹ç‡ï¼‰
    max_epochs: int = 100,
    batch_size: int = 32,
    num_workers: int = 4,
) -> None:
    """PaDiMãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’"""
    logger = logging.getLogger(__name__)

    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    Path(model_save_path).parent.mkdir(parents=True, exist_ok=True)

    logger.info("PaDiMãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™")
    logger.info("å…ƒç”»åƒã‚µã‚¤ã‚º: 640x480 (ã‚«ãƒ¡ãƒ©è§£åƒåº¦)")
    logger.info(f"ãƒªã‚µã‚¤ã‚ºå¾Œã‚µã‚¤ã‚º: {image_size} (å‡¦ç†åŠ¹ç‡ã®ãŸã‚)")
    logger.info(f"æœ€å¤§ã‚¨ãƒãƒƒã‚¯æ•°: {max_epochs}")
    logger.info(f"ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size}")
    logger.info(f"ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {num_workers}")

    # çµ±åˆå­¦ç¿’ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆã¾ãŸã¯å†åˆ©ç”¨
    training_dir = "temp_training_data"

    try:
        # å…¨ç”»åƒã‚’çµ±åˆã—ãŸä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆï¼ˆæ—¢å­˜ã®å ´åˆã¯å†åˆ©ç”¨ï¼‰
        training_root, _, total_images = create_unified_training_dir(
            images_dir, training_dir, image_size
        )

        if total_images == 0:
            logger.error("å­¦ç¿’ã«ä½¿ç”¨ã§ãã‚‹æ­£å¸¸ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            logger.info("ä»¥ä¸‹ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç”»åƒã‚’é…ç½®ã—ã¦ãã ã•ã„:")
            logger.info("  - images/grid_00 ã€œ images/grid_15 (äººãŒå†™ã£ã¦ã„ã‚‹æ­£å¸¸ç”»åƒ)")
            logger.info("  - images/no_person (äººãŒå†™ã£ã¦ã„ãªã„æ­£å¸¸ç”»åƒ)")
            cleanup_training_dir(training_dir)
            return

        if total_images < 10:
            logger.error(
                f"å­¦ç¿’ã«ã¯æœ€ä½10æšã®ç”»åƒãŒå¿…è¦ã§ã™ãŒã€{total_images}æšã—ã‹ã‚ã‚Šã¾ã›ã‚“"
            )
            cleanup_training_dir(training_dir)
            return

        # Folderãƒ‡ãƒ¼ã‚¿ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ä½¿ç”¨ï¼ˆå­¦ç¿’ã¯æ­£å¸¸ç”»åƒã®ã¿ï¼‰
        # ç’°å¢ƒã«å¿œã˜ãŸãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã¨ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’è¨­å®š
        import os

        cpu_count = os.cpu_count() or 1
        optimal_workers = min(4, max(0, cpu_count - 1))  # CPUæ•°-1ã€æœ€å¤§4

        # Dockerã‚„ã‚³ãƒ³ãƒ†ãƒŠç’°å¢ƒã§ã¯å®‰å®šæ€§ã®ãŸã‚ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’åˆ¶é™
        if os.path.exists("/.dockerenv") or os.environ.get("CONTAINER"):
            optimal_workers = 0
            logger.info("Docker/ã‚³ãƒ³ãƒ†ãƒŠç’°å¢ƒã‚’æ¤œå‡º - num_workers=0ã«è¨­å®š")
        else:
            logger.info(f"CPUæ•°: {cpu_count}, ä½¿ç”¨ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {optimal_workers}")

        # ãƒªã‚µã‚¤ã‚ºæ¸ˆã¿ç”»åƒä½¿ç”¨ã®ãŸã‚ã€æ¨™æº–çš„ãªãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’ä½¿ç”¨
        adjusted_batch_size = min(
            batch_size, max(1, total_images // 10)
        )  # æœ€ä½1ã€æœ€å¤§ã§ã‚‚å…¨ãƒ‡ãƒ¼ã‚¿ã®1/10
        logger.info(
            f"èª¿æ•´å¾Œãƒãƒƒãƒã‚µã‚¤ã‚º: {adjusted_batch_size} (å…ƒç”»åƒ640x480â†’{image_size}ã«ãƒªã‚µã‚¤ã‚ºæ¸ˆã¿, ãƒ‡ãƒ¼ã‚¿é‡: {total_images})"
        )

        datamodule = Folder(
            name="padim_training",
            root=training_root,
            normal_dir="normal",
            abnormal_dir="normal",  # ç•°å¸¸æ¤œçŸ¥ã§ã¯ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ã¯ä¸è¦ã€normalã‚’æŒ‡å®š
            train_batch_size=adjusted_batch_size,
            eval_batch_size=adjusted_batch_size,
            num_workers=optimal_workers,  # ç’°å¢ƒã«å¿œã˜ã¦æœ€é©åŒ–
            val_split_ratio=0.2,  # æ­£å¸¸ç”»åƒã®20%ã‚’æ¤œè¨¼ã«ä½¿ç”¨
        )
        logger.info(
            f"Folderãƒ‡ãƒ¼ã‚¿ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ä½œæˆã—ã¾ã—ãŸ (num_workers={optimal_workers})"
        )
        logger.info(f"ç”»åƒã¯æ—¢ã«{image_size}ã«ãƒªã‚µã‚¤ã‚ºæ¸ˆã¿ã§ã™")

        # å®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’å…ˆã«ç¢ºèª
        actual_files = len(
            [f for f in (Path(training_root) / "normal").iterdir() if f.is_file()]
        )
        logger.info(f"temp_training_data/normalå†…ã®å®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {actual_files}")

        # ãƒ‡ãƒ¼ã‚¿ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
        logger.info("ãƒ‡ãƒ¼ã‚¿ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸­...")
        datamodule.setup()
        logger.info("ãƒ‡ãƒ¼ã‚¿ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãŒå®Œäº†ã—ã¾ã—ãŸ")

        # ãƒ‡ãƒãƒƒã‚°: å®Ÿéš›ã«ãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
        try:
            train_loader = datamodule.train_dataloader()
            val_loader = datamodule.val_dataloader()

            train_size = len(train_loader) if train_loader else 0
            val_size = len(val_loader) if val_loader else 0

            logger.info(f"å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚º: {train_size} ãƒãƒƒãƒ")
            logger.info(f"æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚º: {val_size} ãƒãƒƒãƒ")

            # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ãŒç©ºã§ãªã„ã“ã¨ã‚’ç¢ºèª
            if train_size == 0:
                logger.error("å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ãŒç©ºã§ã™")
                raise ValueError("å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ãŒç©ºã®ãŸã‚ã€å­¦ç¿’ã‚’å®Ÿè¡Œã§ãã¾ã›ã‚“")

            # ãƒªã‚µã‚¤ã‚ºæ¸ˆã¿ç”»åƒã«ã‚ˆã‚Šå®‰å®šæ€§ãŒå‘ä¸Š

            # æœ€åˆã®ãƒãƒƒãƒã‚’è©¦é¨“çš„ã«èª­ã¿è¾¼ã‚“ã§æ¤œè¨¼
            try:
                first_batch = next(iter(train_loader))
                # Anomalibã®æ–°ã—ã„ImageBatchã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¯¾å¿œ
                if hasattr(first_batch, "image"):
                    batch_shape = first_batch.image.shape
                    logger.info(f"æœ€åˆã®ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_shape}")
                elif hasattr(first_batch, "keys") and "image" in first_batch:
                    batch_shape = first_batch["image"].shape
                    logger.info(f"æœ€åˆã®ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_shape}")
                else:
                    logger.info(f"æœ€åˆã®ãƒãƒƒãƒ: {type(first_batch)} ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ")
                logger.info("ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®å‹•ä½œç¢ºèªå®Œäº†")
            except Exception as batch_e:
                logger.warning(f"ãƒãƒƒãƒèª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {batch_e}")
                logger.info(
                    "ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã¯æ­£å¸¸ã«ä½œæˆã•ã‚Œã¾ã—ãŸï¼ˆãƒãƒƒãƒãƒ†ã‚¹ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰"
                )

        except Exception as debug_e:
            logger.warning(f"ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ãƒ‡ãƒãƒƒã‚°ä¸­ã«ã‚¨ãƒ©ãƒ¼: {debug_e}")
            logger.info("ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã¯ä½œæˆã•ã‚Œã¾ã—ãŸã€‚å­¦ç¿’ã‚’ç¶šè¡Œã—ã¾ã™ã€‚")

    except Exception as e:
        logger.error(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™ã«å¤±æ•—: {e}")
        # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯å­¦ç¿’ã‚’ç¶šè¡Œ
        if "ImageBatch" in str(e) or "subscriptable" in str(e):
            logger.info("ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼ã‚’ç„¡è¦–ã—ã¦å­¦ç¿’ã‚’ç¶šè¡Œã—ã¾ã™")
        else:
            # ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼ã§ã‚‚ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¯ä¿æŒï¼ˆãƒ‡ãƒãƒƒã‚°ã®ãŸã‚ï¼‰
            logger.info(
                f"ä¸€æ™‚å­¦ç¿’ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä¿æŒã—ã¾ã™ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰: {training_dir}"
            )
            return

    # PyTorchã®ãƒ†ãƒ³ã‚½ãƒ«ç²¾åº¦è¨­å®šï¼ˆTensor Coresã®è­¦å‘Šå¯¾å¿œï¼‰
    torch.set_float32_matmul_precision("medium")
    logger.info("PyTorchã®float32è¡Œåˆ—ä¹—ç®—ç²¾åº¦ã‚’mediumã«è¨­å®šã—ã¾ã—ãŸ")

    # ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™ï¼ˆç”»åƒã‚µã‚¤ã‚ºã‚’æ˜ç¤ºçš„ã«æŒ‡å®šï¼‰
    logger.info(f"PaDiMãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆä¸­ï¼ˆç”»åƒã‚µã‚¤ã‚º: {image_size}ï¼‰")
    model = create_padim_model(image_size=image_size)

    # ç”»åƒã¯æ—¢ã«ãƒªã‚µã‚¤ã‚ºæ¸ˆã¿
    logger.info("ãƒªã‚µã‚¤ã‚ºæ¸ˆã¿ç”»åƒã‚’ä½¿ç”¨ã—ã¾ã™")

    # ã‚«ã‚¹ã‚¿ãƒ ãƒ­ã‚¬ãƒ¼ã¨ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã®è¨­å®š
    try:
        from lightning.pytorch.loggers import TensorBoardLogger
        from lightning.pytorch.callbacks import (
            ModelCheckpoint,
            EarlyStopping,
            ProgressBar,
        )
    except ImportError:
        # fallback for older versions
        from pytorch_lightning.loggers import TensorBoardLogger
        from pytorch_lightning.callbacks import (
            ModelCheckpoint,
            EarlyStopping,
            ProgressBar,
        )

    # TensorBoardãƒ­ã‚¬ãƒ¼
    tb_logger = TensorBoardLogger(
        save_dir="lightning_logs", name="padim_training", version=None
    )

    # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
    try:
        progress_bar = ProgressBar(refresh_rate=1)  # å¤ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç”¨
    except TypeError:
        try:
            progress_bar = ProgressBar(refresh_rate_per_second=1)  # æ–°ã—ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç”¨
        except TypeError:
            progress_bar = ProgressBar()  # å¼•æ•°ãªã—ã§ä½œæˆ

    # ãƒ¢ãƒ‡ãƒ«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
    checkpoint_callback = ModelCheckpoint(
        dirpath="lightning_logs/checkpoints",
        filename="padim-{epoch:02d}-{val_loss:.2f}",
        save_top_k=3,
        monitor="val_loss",
        mode="min",
        save_last=True,
        verbose=True,
    )

    # æ—©æœŸåœæ­¢ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
    early_stop_callback = EarlyStopping(
        monitor="val_loss", patience=10, mode="min", verbose=True
    )

    # Trainerã®æº–å‚™ï¼ˆè©³ç´°ãƒ­ã‚°è¨­å®šï¼‰
    trainer = pl.Trainer(
        max_epochs=max_epochs,
        accelerator="auto",
        devices="auto",
        logger=tb_logger,
        log_every_n_steps=5,  # ã‚ˆã‚Šé »ç¹ã«ãƒ­ã‚°å‡ºåŠ›
        enable_checkpointing=True,
        callbacks=[progress_bar, checkpoint_callback, early_stop_callback],
        default_root_dir="lightning_logs",
        enable_progress_bar=True,
        enable_model_summary=True,
        profiler="simple",  # ã‚·ãƒ³ãƒ—ãƒ«ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ã‚’æœ‰åŠ¹åŒ–
    )

    # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼æƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›
    logger.info("=" * 30)
    logger.info("Trainerã®è¨­å®š")
    logger.info("=" * 30)
    logger.info(f"æœ€å¤§ã‚¨ãƒãƒƒã‚¯æ•°: {trainer.max_epochs}")
    logger.info(f"ãƒ­ã‚°å‡ºåŠ›é–“éš”: {trainer.log_every_n_steps} ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨")
    logger.info("ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜: æœ‰åŠ¹")
    logger.info("æ—©æœŸåœæ­¢: æœ‰åŠ¹ (patience=10)")
    logger.info("TensorBoardãƒ­ã‚°: lightning_logs/padim_training")
    logger.info("=" * 30)

    # å­¦ç¿’å®Ÿè¡Œ
    logger.info("=" * 50)
    logger.info("PaDiMãƒ¢ãƒ‡ãƒ«å­¦ç¿’é–‹å§‹")
    logger.info("=" * 50)
    logger.info(f"å­¦ç¿’é–‹å§‹æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(
        f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {trainer.device_ids if hasattr(trainer, 'device_ids') else 'auto'}"
    )
    logger.info(f"ã‚¢ã‚¯ã‚»ãƒ©ãƒ¬ãƒ¼ã‚¿: {trainer.accelerator}")
    logger.info(
        f"å­¦ç¿’ãƒ‡ãƒ¼ã‚¿é‡: {len(train_loader) if 'train_loader' in locals() else 'unknown'} ãƒãƒƒãƒ"
    )
    logger.info(
        f"æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿é‡: {len(val_loader) if 'val_loader' in locals() else 'unknown'} ãƒãƒƒãƒ"
    )
    logger.info("ãƒ¢ãƒ‡ãƒ«ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³: resnet18")
    logger.info("ç‰¹å¾´æŠ½å‡ºãƒ¬ã‚¤ãƒ¤ãƒ¼: ['layer1', 'layer2', 'layer3']")
    logger.info("=" * 50)

    # å­¦ç¿’å®Ÿè¡Œï¼ˆè©³ç´°ãƒ­ã‚°ä»˜ãï¼‰
    try:
        trainer.fit(model=model, datamodule=datamodule)
        logger.info("=" * 50)
        logger.info("å­¦ç¿’ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ")
        logger.info(f"å­¦ç¿’å®Œäº†æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info("=" * 50)
    except Exception as e:
        logger.error("=" * 50)
        logger.error("å­¦ç¿’ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
        logger.error(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {e}")
        logger.error("=" * 50)
        raise

    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    logger.info("=" * 30)
    logger.info("ãƒ¢ãƒ‡ãƒ«ä¿å­˜é–‹å§‹")
    logger.info("=" * 30)
    logger.info(f"ä¿å­˜ãƒ‘ã‚¹: {model_save_path}")

    try:
        trainer.save_checkpoint(model_save_path)
        model_size = Path(model_save_path).stat().st_size / (1024 * 1024)  # MB
        logger.info(f"ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜å®Œäº†: {model_size:.2f} MB")

        # è¿½åŠ ã§.save()å½¢å¼ã§ã‚‚ä¿å­˜
        save_dir = Path(model_save_path).parent / "padim_saved_model"
        save_dir.mkdir(exist_ok=True)
        model.model.save(str(save_dir))
        logger.info(f"ãƒ¢ãƒ‡ãƒ«ï¼ˆ.save()å½¢å¼ï¼‰ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {save_dir}")

        # ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°æƒ…å ±
        if Path(model_save_path).exists():
            stat = Path(model_save_path).stat()
            logger.info(f"ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {stat.st_size / (1024 * 1024):.2f} MB")
            logger.info(
                f"ä¿å­˜æ—¥æ™‚: {datetime.fromtimestamp(stat.st_mtime).strftime('%Y-%m-%d %H:%M:%S')}"
            )

        logger.info("=" * 30)
        logger.info("ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†")
        logger.info("=" * 30)

    except Exception as e:
        logger.error("=" * 30)
        logger.error("ãƒ¢ãƒ‡ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼")
        logger.error(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {e}")
        logger.error("=" * 30)
        raise

    logger.info("ğŸ‰ PaDiMãƒ¢ãƒ‡ãƒ«å­¦ç¿’ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ ğŸ‰")

    # ä¸€æ™‚å­¦ç¿’ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‰Šé™¤ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    # main.pyã§ã®æ¨è«–é«˜é€ŸåŒ–ã®ãŸã‚ã€temp_training_dataãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä¿æŒ
    logger.info(f"å­¦ç¿’ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä¿æŒã—ã¾ã™ï¼ˆæ¨è«–é«˜é€ŸåŒ–ã®ãŸã‚ï¼‰: {training_dir}")
    # cleanup_training_dir(training_dir)  # ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã—ã¦ä¿æŒ


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description="PaDiMç•°å¸¸æ¤œçŸ¥ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’")
    parser.add_argument(
        "--images-dir",
        type=str,
        default="images",
        help="ç”»åƒãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ (default: images)",
    )
    parser.add_argument(
        "--model-path",
        type=str,
        default="models/padim_model.ckpt",
        help="ãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ‘ã‚¹ (default: models/padim_model.ckpt)",
    )
    parser.add_argument(
        "--image-size",
        type=int,
        nargs=2,
        default=[224, 224],
        help="ãƒªã‚µã‚¤ã‚ºå¾Œã®ç”»åƒã‚µã‚¤ã‚º (width height) (default: 224 224)",
    )
    parser.add_argument(
        "--max-epochs", type=int, default=100, help="æœ€å¤§ã‚¨ãƒãƒƒã‚¯æ•° (default: 100)"
    )
    parser.add_argument(
        "--batch-size", type=int, default=32, help="ãƒãƒƒãƒã‚µã‚¤ã‚º (default: 32)"
    )
    parser.add_argument(
        "--num-workers",
        type=int,
        default=4,
        help="ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ãƒ¯ãƒ¼ã‚«ãƒ¼æ•° (default: 4)",
    )
    parser.add_argument(
        "--check-only", action="store_true", help="ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®ãƒã‚§ãƒƒã‚¯ã®ã¿å®Ÿè¡Œ"
    )
    parser.add_argument(
        "--cleanup",
        action="store_true",
        help="å­¦ç¿’å¾Œã«temp_training_dataãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‰Šé™¤",
    )
    parser.add_argument(
        "--force-recreate",
        action="store_true",
        help="æ—¢å­˜ã®temp_training_dataãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å¼·åˆ¶çš„ã«å†ä½œæˆ",
    )

    args = parser.parse_args()

    # ãƒ­ã‚°è¨­å®š
    logger = setup_logging()

    try:
        logger.info("PaDiMå­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’é–‹å§‹ã—ã¾ã™")
        logger.info(f"ç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {args.images_dir}")

        # ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®ç¢ºèª
        data_structure = check_data_structure(args.images_dir)

        logger.info("=== ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹é€  ===")
        logger.info(f"ã‚°ãƒªãƒƒãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ•°: {len(data_structure['grid_dirs'])}")
        for grid_dir in data_structure["grid_dirs"]:
            count = count_images_in_directory(grid_dir)
            logger.info(f"  {grid_dir.name}: {count} ç”»åƒ")

        if data_structure["no_person_dir"]:
            no_person_count = count_images_in_directory(data_structure["no_person_dir"])
            logger.info(f"no_person: {no_person_count} ç”»åƒ")
        else:
            logger.warning("no_personãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

        if data_structure["test_dir"]:
            # testãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã”ã¨ã«ç”»åƒæ•°ã‚’è¡¨ç¤º
            if data_structure["test_normal_dir"]:
                test_normal_count = count_images_in_directory(
                    data_structure["test_normal_dir"]
                )
                logger.info(f"test/normal: {test_normal_count} ç”»åƒ")

            if data_structure["test_anomaly_dir"]:
                test_anomaly_count = count_images_in_directory(
                    data_structure["test_anomaly_dir"]
                )
                logger.info(f"test/anomaly: {test_anomaly_count} ç”»åƒ")

            # å…¨ä½“ã®testç”»åƒæ•°ã‚‚è¡¨ç¤º
            test_count = count_images_in_directory(data_structure["test_dir"])
            logger.info(f"test (åˆè¨ˆ): {test_count} ç”»åƒ")
        else:
            logger.warning("testãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

        # ãƒ‡ãƒ¼ã‚¿æ§‹é€ ãƒã‚§ãƒƒã‚¯ã®ã¿ã®å ´åˆã¯çµ‚äº†
        if args.check_only:
            logger.info("ãƒ‡ãƒ¼ã‚¿æ§‹é€ ãƒã‚§ãƒƒã‚¯ãŒå®Œäº†ã—ã¾ã—ãŸ")
            return 0

        # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        total_grid_images = sum(
            count_images_in_directory(grid_dir)
            for grid_dir in data_structure["grid_dirs"]
        )
        no_person_images = (
            count_images_in_directory(data_structure["no_person_dir"])
            if data_structure["no_person_dir"]
            else 0
        )
        total_normal_images = total_grid_images + no_person_images

        if total_normal_images == 0:
            logger.error("å­¦ç¿’ç”¨ã®æ­£å¸¸ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return 1

        logger.info(f"åˆè¨ˆæ­£å¸¸ç”»åƒæ•°: {total_normal_images}")

        # å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã®æƒ…å ±ã‚’å–å¾—ï¼ˆãƒ­ã‚°é‡è¤‡å›é¿ã®ãŸã‚ç°¡ç•¥åŒ–ï¼‰
        normal_count = total_normal_images

        if normal_count == 0:
            logger.error("æ­£å¸¸ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return 1

        # --force-recreateã‚ªãƒ—ã‚·ãƒ§ãƒ³ãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆã¯æ—¢å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‰Šé™¤
        if args.force_recreate and Path("temp_training_data").exists():
            cleanup_training_dir("temp_training_data")
            logger.info(
                "--force-recreateã‚ªãƒ—ã‚·ãƒ§ãƒ³ã«ã‚ˆã‚Šã€æ—¢å­˜ã®temp_training_dataãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‰Šé™¤ã—ã¾ã—ãŸ"
            )

        # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã®å®Ÿè¡Œ
        train_padim_model(
            images_dir=args.images_dir,
            model_save_path=args.model_path,
            image_size=tuple(args.image_size),
            max_epochs=args.max_epochs,
            batch_size=args.batch_size,
            num_workers=args.num_workers,
        )

        # --cleanupã‚ªãƒ—ã‚·ãƒ§ãƒ³ãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆã®ã¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‰Šé™¤
        if args.cleanup:
            cleanup_training_dir("temp_training_data")
            logger.info(
                "--cleanupã‚ªãƒ—ã‚·ãƒ§ãƒ³ã«ã‚ˆã‚Šã€temp_training_dataãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‰Šé™¤ã—ã¾ã—ãŸ"
            )

        logger.info("ã™ã¹ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ")
        return 0

    except Exception as e:
        logger.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return 1


if __name__ == "__main__":
    sys.exit(main())
