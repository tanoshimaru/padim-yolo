#!/usr/bin/env python3
"""
PaDiM ç•°å¸¸æ¤œçŸ¥ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

å­¦ç¿’ãƒ‡ãƒ¼ã‚¿:
- images/grid_XX (00-15): æ­£å¸¸ç”»åƒ (äººãŒå†™ã£ã¦ã„ã‚‹æ­£å¸¸ãªã‚°ãƒªãƒƒãƒ‰åˆ¥ç”»åƒ)
- images/no_person: æ­£å¸¸ç”»åƒ (äººãŒå†™ã£ã¦ã„ãªã„ç”»åƒ)

æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿:
- images/test/normal: æ­£å¸¸ãªæ¤œè¨¼ç”»åƒ
- images/test/anomaly: ç•°å¸¸ãªæ¤œè¨¼ç”»åƒ

å­¦ç¿’ã¯æ­£å¸¸ç”»åƒã®ã¿ã§è¡Œã„ã€ç•°å¸¸æ¤œçŸ¥ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚
testãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®normal/anomalyã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¯æ¨è«–è©•ä¾¡æ™‚ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚
"""

import sys
import logging
from datetime import datetime
from pathlib import Path
from typing import List
import argparse
import torch


from anomalib.data import Folder
from anomalib.engine import Engine
from anomalib.models import Padim
import shutil

# Jetson Orin GPUå‘ã‘ã®æœ€é©åŒ–è¨­å®š
torch.set_float32_matmul_precision("high")


def create_unified_training_dir(
    images_dir: str,
    training_dir: str = "dataset",
    image_size: tuple = (224, 224),
) -> tuple:
    """å…¨ã¦ã®æ­£å¸¸ç”»åƒã‚’çµ±åˆã—ãŸå­¦ç¿’ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆï¼ˆã‚·ã‚§ãƒ«ã‚¹ã‚¯ãƒªãƒ—ãƒˆä½¿ç”¨ï¼‰"""
    import subprocess

    logger = logging.getLogger(__name__)

    training_path = Path(training_dir)
    train_good_dir = training_path / "train" / "good"
    test_good_dir = training_path / "test" / "good"
    test_defect_dir = training_path / "test" / "defect"

    # æ—¢å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒã‚ã‚‹å ´åˆã€ãƒªã‚µã‚¤ã‚ºæ¸ˆã¿ã‹ã©ã†ã‹ã‚’ç¢ºèª
    if train_good_dir.exists():
        existing_images = list(train_good_dir.glob("*"))
        if test_good_dir.exists():
            existing_images.extend(list(test_good_dir.glob("*")))
        if test_defect_dir.exists():
            existing_images.extend(list(test_defect_dir.glob("*")))
        
        existing_image_files = [
            f
            for f in existing_images
            if f.is_file()
            and f.suffix.lower() in {".jpg", ".jpeg", ".png", ".bmp", ".tiff", ".tif"}
        ]

        # æ—¢å­˜ç”»åƒã®ã‚µã‚¤ã‚ºã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆæœ€åˆã®ç”»åƒã§åˆ¤å®šï¼‰
        if existing_image_files:
            from PIL import Image

            try:
                sample_image = Image.open(existing_image_files[0])
                current_size = sample_image.size
                target_size = (image_size[0], image_size[1])

                if current_size == target_size and len(existing_image_files) >= 10:
                    logger.info(
                        f"æ—¢å­˜ã®ç”»åƒã¯æ—¢ã«{target_size}ã«ãƒªã‚µã‚¤ã‚ºæ¸ˆã¿ã§ã™: {len(existing_image_files)} ç”»åƒ"
                    )
                    return str(training_path), len(existing_image_files)
                else:
                    logger.info(
                        f"æ—¢å­˜ç”»åƒã‚µã‚¤ã‚º: {current_size} â†’ ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ: {target_size} - å†ãƒªã‚µã‚¤ã‚ºãŒå¿…è¦"
                    )
                    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã—ã¦å†ä½œæˆ
                    for file in existing_image_files:
                        file.unlink()
            except Exception as e:
                logger.warning(f"æ—¢å­˜ç”»åƒã®ã‚µã‚¤ã‚ºç¢ºèªã‚¨ãƒ©ãƒ¼: {e} - å†ä½œæˆã—ã¾ã™")
                # ã‚¨ãƒ©ãƒ¼æ™‚ã¯æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
                for file in existing_image_files:
                    file.unlink()
        else:
            logger.info("æ—¢å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¯ç©ºã§ã™ - æ–°è¦ä½œæˆ")
    else:
        logger.info("æ–°è¦ã§datasetãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ")

    # ã‚·ã‚§ãƒ«ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§é«˜é€Ÿãƒªã‚µã‚¤ã‚º&ã‚³ãƒ”ãƒ¼ã‚’å®Ÿè¡Œ
    script_path = Path(__file__).parent / "create_dataset_structure.sh"
    target_size = f"{image_size[0]}x{image_size[1]}"  # 224x224å½¢å¼
    try:
        result = subprocess.run(
            [str(script_path), images_dir, training_dir, target_size],
            capture_output=True,
            text=True,
            check=True,
        )

        # æœ€å¾Œã®è¡Œã‹ã‚‰ç”»åƒæ•°ã‚’å–å¾—
        output_lines = result.stdout.strip().split("\n")
        for line in output_lines[:-1]:  # æœ€å¾Œã®è¡Œä»¥å¤–ã‚’å‡ºåŠ›
            logger.info(line)

        total_images = int(output_lines[-1]) if output_lines else 0

    except subprocess.CalledProcessError as e:
        logger.error(f"ã‚·ã‚§ãƒ«ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e.stderr}")
        return str(training_path), 0
    except (ValueError, IndexError):
        logger.error("ã‚·ã‚§ãƒ«ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‹ã‚‰ã®å‡ºåŠ›è§£æã‚¨ãƒ©ãƒ¼")
        return str(training_path), 0

    logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆå®Œäº†: {total_images} æš")
    return str(training_path), total_images


def cleanup_training_dir(training_dir: str):
    """å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‰Šé™¤"""
    logger = logging.getLogger(__name__)

    training_path = Path(training_dir)
    if training_path.exists():
        shutil.rmtree(training_path)
        logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‰Šé™¤ã—ã¾ã—ãŸ: {training_dir}")


def setup_logging() -> logging.Logger:
    """ãƒ­ã‚°è¨­å®š"""
    log_dir = Path("logs")
    log_dir.mkdir(exist_ok=True)

    log_filename = f"train_padim_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s",
        handlers=[
            logging.FileHandler(log_dir / log_filename, encoding="utf-8"),
            logging.StreamHandler(sys.stdout),
        ],
    )
    return logging.getLogger(__name__)


def check_data_structure(images_dir: str) -> dict:
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹é€ ã®ç¢ºèª"""
    images_path = Path(images_dir)

    # ã‚°ãƒªãƒƒãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºèª
    grid_dirs = []
    for i in range(8):  # grid_00 ã‹ã‚‰ grid_15
        grid_dir = images_path / f"grid_{i:02d}"
        if grid_dir.exists() and grid_dir.is_dir():
            grid_dirs.append(grid_dir)

    # no_personãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºèª
    no_person_dir = images_path / "no_person"

    # testãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºèªï¼ˆnormal/anomalyã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå«ã‚€ï¼‰
    test_dir = images_path / "test"
    test_normal_dir = test_dir / "normal" if test_dir.exists() else None
    test_anomaly_dir = test_dir / "anomaly" if test_dir.exists() else None

    return {
        "grid_dirs": grid_dirs,
        "no_person_dir": no_person_dir if no_person_dir.exists() else None,
        "test_dir": test_dir if test_dir.exists() else None,
        "test_normal_dir": test_normal_dir
        if test_normal_dir and test_normal_dir.exists()
        else None,
        "test_anomaly_dir": test_anomaly_dir
        if test_anomaly_dir and test_anomaly_dir.exists()
        else None,
    }


def count_images_in_directory(directory: Path) -> int:
    """ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
    if not directory.exists():
        return 0

    image_extensions = {".jpg", ".jpeg", ".png", ".bmp", ".tiff", ".tif"}
    count = 0

    for file_path in directory.iterdir():
        if file_path.is_file() and file_path.suffix.lower() in image_extensions:
            count += 1

    return count


def get_training_info(images_dir: str) -> tuple:
    """å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã®æƒ…å ±ã‚’å–å¾—"""
    logger = logging.getLogger(__name__)

    data_structure = check_data_structure(images_dir)

    # æ­£å¸¸ç”»åƒæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
    total_normal_images = 0

    # grid_XX ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰æ­£å¸¸ç”»åƒã‚’ã‚«ã‚¦ãƒ³ãƒˆ
    for grid_dir in data_structure["grid_dirs"]:
        image_count = count_images_in_directory(grid_dir)
        logger.info(f"{grid_dir.name}: {image_count} ç”»åƒ")
        total_normal_images += image_count

    # no_person ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰æ­£å¸¸ç”»åƒã‚’ã‚«ã‚¦ãƒ³ãƒˆ
    if data_structure["no_person_dir"]:
        no_person_count = count_images_in_directory(data_structure["no_person_dir"])
        logger.info(f"no_person: {no_person_count} ç”»åƒ")
        total_normal_images += no_person_count

    # test ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç”»åƒæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
    test_images = 0
    if data_structure["test_dir"]:
        test_images = count_images_in_directory(data_structure["test_dir"])
        logger.info(f"test: {test_images} ç”»åƒ")

    logger.info(f"å­¦ç¿’ç”¨æ­£å¸¸ç”»åƒ: {total_normal_images} æš")
    logger.info(f"æ¤œè¨¼ç”¨ç”»åƒ: {test_images} æš")

    # å®Ÿéš›ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹ã‚’è¿”ã™
    normal_dirs = [str(d) for d in data_structure["grid_dirs"]]
    if data_structure["no_person_dir"]:
        normal_dirs.append(str(data_structure["no_person_dir"]))

    test_dir = str(data_structure["test_dir"]) if data_structure["test_dir"] else None

    return normal_dirs, test_dir, total_normal_images, test_images


def create_padim_model(
    image_size: tuple = (224, 224),  # ResNetæ¨™æº–ã‚µã‚¤ã‚ºï¼ˆæœ€é©ãªå‡¦ç†åŠ¹ç‡ï¼‰
    backbone: str = "resnet18",
    layers: List[str] | None = None,
) -> Padim:
    """PaDiMãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ"""
    if layers is None:
        layers = ["layer1", "layer2", "layer3"]

    pre_processor = Padim.configure_pre_processor(image_size=image_size)

    model = Padim(
        backbone=backbone,
        layers=layers,
        pre_trained=True,
        pre_processor=pre_processor,
        n_features=None,  # è‡ªå‹•ã§ç‰¹å¾´é‡æ•°ã‚’æ±ºå®š
    )

    return model




def train_padim_model(
    images_dir: str,
    model_save_path: str = "models/padim_trained.ckpt",
    image_size: tuple = (224, 224),  # ResNetæ¨™æº–ã‚µã‚¤ã‚ºï¼ˆæœ€é©ãªå‡¦ç†åŠ¹ç‡ï¼‰
    max_epochs: int = 10,
    batch_size: int = 4,  # Jetsonå‘ã‘ã«å‰Šæ¸›
    num_workers: int = 2,  # Jetsonå‘ã‘ã«å‰Šæ¸›
) -> None:
    """PaDiMãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’"""
    logger = logging.getLogger(__name__)

    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    Path(model_save_path).parent.mkdir(parents=True, exist_ok=True)

    logger.info("PaDiMãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™")
    logger.info("å…ƒç”»åƒã‚µã‚¤ã‚º: 640x480 (ã‚«ãƒ¡ãƒ©è§£åƒåº¦)")
    logger.info(f"ãƒªã‚µã‚¤ã‚ºå¾Œã‚µã‚¤ã‚º: {image_size} (å‡¦ç†åŠ¹ç‡ã®ãŸã‚)")
    logger.info(f"æœ€å¤§ã‚¨ãƒãƒƒã‚¯æ•°: {max_epochs}")
    logger.info(f"ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size}")
    logger.info(f"ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {num_workers}")

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆã¾ãŸã¯å†åˆ©ç”¨
    training_dir = "dataset"

    try:
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆï¼ˆæ—¢å­˜ã®å ´åˆã¯å†åˆ©ç”¨ï¼‰
        training_root, total_images = create_unified_training_dir(
            images_dir, training_dir, image_size
        )

        if total_images == 0:
            logger.error("å­¦ç¿’ã«ä½¿ç”¨ã§ãã‚‹ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            logger.info("ä»¥ä¸‹ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç”»åƒã‚’é…ç½®ã—ã¦ãã ã•ã„:")
            logger.info("  - images/grid_00 ã€œ images/grid_15 (äººãŒå†™ã£ã¦ã„ã‚‹æ­£å¸¸ç”»åƒ)")
            logger.info("  - images/no_person (äººãŒå†™ã£ã¦ã„ãªã„æ­£å¸¸ç”»åƒ)")
            logger.info("  - images/test/normal (æ­£å¸¸ãªãƒ†ã‚¹ãƒˆç”»åƒ)")
            logger.info("  - images/test/anomaly (ç•°å¸¸ãªãƒ†ã‚¹ãƒˆç”»åƒ)")
            cleanup_training_dir(training_dir)
            return

        # Folderãƒ‡ãƒ¼ã‚¿ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ä½¿ç”¨ï¼ˆdataset/train/goodã€dataset/test/goodã€dataset/test/defectï¼‰
        datamodule = Folder(
            name="padim_training",
            root=training_root,
            normal_dir="train/good",
            abnormal_dir="test/defect",
            train_batch_size=batch_size,
            eval_batch_size=batch_size,
            num_workers=num_workers,
            test_split_mode="from_dir",
            test_split_ratio=0.0,
            val_split_mode="from_dir", 
            val_split_ratio=0.0,
        )
        logger.info(f"Folderãƒ‡ãƒ¼ã‚¿ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ä½œæˆã—ã¾ã—ãŸ (num_workers={num_workers})")

        # å®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’å…ˆã«ç¢ºèª
        train_files = len(
            [f for f in (Path(training_root) / "train" / "good").iterdir() if f.is_file()]
        )
        test_good_files = len(
            [f for f in (Path(training_root) / "test" / "good").iterdir() if f.is_file()]
        ) if (Path(training_root) / "test" / "good").exists() else 0
        test_defect_files = len(
            [f for f in (Path(training_root) / "test" / "defect").iterdir() if f.is_file()]
        ) if (Path(training_root) / "test" / "defect").exists() else 0
        
        logger.info(f"dataset/train/goodå†…ã®å®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {train_files}")
        logger.info(f"dataset/test/goodå†…ã®å®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {test_good_files}")
        logger.info(f"dataset/test/defectå†…ã®å®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {test_defect_files}")

        # ãƒ‡ãƒ¼ã‚¿ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
        logger.info("ãƒ‡ãƒ¼ã‚¿ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸­...")
        datamodule.setup()
        logger.info("ãƒ‡ãƒ¼ã‚¿ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãŒå®Œäº†ã—ã¾ã—ãŸ")

    except Exception as e:
        logger.error(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™ã«å¤±æ•—: {e}")
        raise

    # ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™ï¼ˆç”»åƒã‚µã‚¤ã‚ºã‚’æ˜ç¤ºçš„ã«æŒ‡å®šï¼‰
    logger.info(f"PaDiMãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆä¸­ï¼ˆç”»åƒã‚µã‚¤ã‚º: {image_size}ï¼‰")
    model = create_padim_model(image_size=image_size)
    engine = Engine(max_epochs=max_epochs)

    # å­¦ç¿’å®Ÿè¡Œ
    logger.info("=" * 50)
    logger.info("PaDiMãƒ¢ãƒ‡ãƒ«å­¦ç¿’é–‹å§‹")
    logger.info("=" * 50)
    logger.info(f"å­¦ç¿’é–‹å§‹æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("ãƒ¢ãƒ‡ãƒ«ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³: resnet18")
    # logger.info(
    #     f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {engine.trainer.device_ids if hasattr(engine.trainer, 'device_ids') else 'auto'}"
    # )
    # logger.info(f"ã‚¢ã‚¯ã‚»ãƒ©ãƒ¬ãƒ¼ã‚¿: {engine.trainer.accelerator}")
    logger.info("ç‰¹å¾´æŠ½å‡ºãƒ¬ã‚¤ãƒ¤ãƒ¼: ['layer1', 'layer2', 'layer3']")
    logger.info("=" * 50)

    try:
        engine.fit(model=model, datamodule=datamodule)
        logger.info("=" * 50)
        logger.info("å­¦ç¿’ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ")
        logger.info(f"å­¦ç¿’å®Œäº†æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info("=" * 50)
    except Exception as e:
        logger.error("=" * 50)
        logger.error("å­¦ç¿’ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
        logger.error(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {e}")
        logger.error("=" * 50)
        raise

    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    logger.info("=" * 30)
    logger.info("ãƒ¢ãƒ‡ãƒ«ä¿å­˜é–‹å§‹")
    logger.info("=" * 30)
    logger.info(f"ä¿å­˜ãƒ‘ã‚¹: {model_save_path}")

    try:
        engine.trainer.save_checkpoint(model_save_path)
        model_size = Path(model_save_path).stat().st_size / (1024 * 1024)  # MB
        logger.info(f"ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜å®Œäº†: {model_size:.2f} MB")
        logger.info("=" * 30)
        logger.info("ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†")
        logger.info("=" * 30)

    except Exception as e:
        logger.error("=" * 30)
        logger.error("ãƒ¢ãƒ‡ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼")
        logger.error(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {e}")
        logger.error("=" * 30)
        raise

    # ãƒ†ã‚¹ãƒˆã¯æ—¢å­˜ã®datamoduleã§å®Ÿè¡Œ
    logger.info("=" * 30)
    logger.info("ãƒ†ã‚¹ãƒˆé–‹å§‹")
    logger.info("=" * 30)

    try:
        # testã‚’å®Ÿè¡Œ
        logger.info("ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
        test_results = engine.test(model=model, datamodule=datamodule)

        logger.info("=" * 30)
        logger.info("ãƒ†ã‚¹ãƒˆå®Œäº†")
        logger.info(f"ãƒ†ã‚¹ãƒˆçµæœ: {test_results}")
        logger.info("=" * 30)

    except Exception as e:
        logger.error(f"ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
        logger.warning("ãƒ†ã‚¹ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸãŒã€å­¦ç¿’ã¯æ­£å¸¸ã«å®Œäº†ã—ã¦ã„ã¾ã™")

    logger.info("ğŸ‰ PaDiMãƒ¢ãƒ‡ãƒ«å­¦ç¿’ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ ğŸ‰")


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description="PaDiMç•°å¸¸æ¤œçŸ¥ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’")
    parser.add_argument(
        "--images-dir",
        type=str,
        default="images",
        help="ç”»åƒãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ (default: images)",
    )
    parser.add_argument(
        "--model-path",
        type=str,
        default="models/padim_trained.ckpt",
        help="ãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ‘ã‚¹ (default: models/padim_trained.ckpt)",
    )
    parser.add_argument(
        "--image-size",
        type=int,
        nargs=2,
        default=[224, 224],
        help="ãƒªã‚µã‚¤ã‚ºå¾Œã®ç”»åƒã‚µã‚¤ã‚º (width height) (default: 224 224)",
    )
    parser.add_argument(
        "--max-epochs", type=int, default=100, help="æœ€å¤§ã‚¨ãƒãƒƒã‚¯æ•° (default: 100)"
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=4,
        help="ãƒãƒƒãƒã‚µã‚¤ã‚º (default: 4 - Jetsonæœ€é©åŒ–)",
    )
    parser.add_argument(
        "--num-workers",
        type=int,
        default=2,
        help="ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ãƒ¯ãƒ¼ã‚«ãƒ¼æ•° (default: 2 - Jetsonæœ€é©åŒ–)",
    )
    parser.add_argument(
        "--check-only", action="store_true", help="ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®ãƒã‚§ãƒƒã‚¯ã®ã¿å®Ÿè¡Œ"
    )
    parser.add_argument(
        "--cleanup",
        action="store_true",
        help="å­¦ç¿’å¾Œã«datasetãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‰Šé™¤",
    )
    parser.add_argument(
        "--force-recreate",
        action="store_true",
        help="æ—¢å­˜ã®datasetãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å¼·åˆ¶çš„ã«å†ä½œæˆ",
    )

    args = parser.parse_args()

    # ãƒ­ã‚°è¨­å®š
    logger = setup_logging()

    try:
        logger.info("PaDiMå­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’é–‹å§‹ã—ã¾ã™")
        logger.info(f"ç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {args.images_dir}")

        # ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®ç¢ºèª
        data_structure = check_data_structure(args.images_dir)

        logger.info("=== ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹é€  ===")
        logger.info(f"ã‚°ãƒªãƒƒãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ•°: {len(data_structure['grid_dirs'])}")
        for grid_dir in data_structure["grid_dirs"]:
            count = count_images_in_directory(grid_dir)
            logger.info(f"  {grid_dir.name}: {count} ç”»åƒ")

        if data_structure["no_person_dir"]:
            no_person_count = count_images_in_directory(data_structure["no_person_dir"])
            logger.info(f"no_person: {no_person_count} ç”»åƒ")
        else:
            logger.warning("no_personãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

        if data_structure["test_dir"]:
            # testãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã”ã¨ã«ç”»åƒæ•°ã‚’è¡¨ç¤º
            test_normal_count = 0
            test_anomaly_count = 0

            if data_structure["test_normal_dir"]:
                test_normal_count = count_images_in_directory(
                    data_structure["test_normal_dir"]
                )
                logger.info(f"test/normal: {test_normal_count} ç”»åƒ")

            if data_structure["test_anomaly_dir"]:
                test_anomaly_count = count_images_in_directory(
                    data_structure["test_anomaly_dir"]
                )
                logger.info(f"test/anomaly: {test_anomaly_count} ç”»åƒ")

            # å…¨ä½“ã®testç”»åƒæ•°ã‚’è¨ˆç®—ï¼ˆã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®åˆè¨ˆï¼‰
            test_count = test_normal_count + test_anomaly_count
            logger.info(f"test (åˆè¨ˆ): {test_count} ç”»åƒ")
        else:
            logger.warning("testãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

        # ãƒ‡ãƒ¼ã‚¿æ§‹é€ ãƒã‚§ãƒƒã‚¯ã®ã¿ã®å ´åˆã¯çµ‚äº†
        if args.check_only:
            logger.info("ãƒ‡ãƒ¼ã‚¿æ§‹é€ ãƒã‚§ãƒƒã‚¯ãŒå®Œäº†ã—ã¾ã—ãŸ")
            return 0

        # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        total_grid_images = sum(
            count_images_in_directory(grid_dir)
            for grid_dir in data_structure["grid_dirs"]
        )
        no_person_images = (
            count_images_in_directory(data_structure["no_person_dir"])
            if data_structure["no_person_dir"]
            else 0
        )
        total_normal_images = total_grid_images + no_person_images

        if total_normal_images == 0:
            logger.error("å­¦ç¿’ç”¨ã®æ­£å¸¸ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return 1

        logger.info(f"åˆè¨ˆæ­£å¸¸ç”»åƒæ•°: {total_normal_images}")

        # å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã®æƒ…å ±ã‚’å–å¾—ï¼ˆãƒ­ã‚°é‡è¤‡å›é¿ã®ãŸã‚ç°¡ç•¥åŒ–ï¼‰
        normal_count = total_normal_images

        if normal_count == 0:
            logger.error("æ­£å¸¸ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return 1

        # --force-recreateã‚ªãƒ—ã‚·ãƒ§ãƒ³ãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆã¯æ—¢å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‰Šé™¤
        if args.force_recreate and Path("dataset").exists():
            cleanup_training_dir("dataset")
            logger.info(
                "--force-recreateã‚ªãƒ—ã‚·ãƒ§ãƒ³ã«ã‚ˆã‚Šã€æ—¢å­˜ã®datasetãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‰Šé™¤ã—ã¾ã—ãŸ"
            )

        # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã®å®Ÿè¡Œ
        train_padim_model(
            images_dir=args.images_dir,
            model_save_path=args.model_path,
            image_size=tuple(args.image_size),
            max_epochs=args.max_epochs,
            batch_size=args.batch_size,
            num_workers=args.num_workers,
        )

        # --cleanupã‚ªãƒ—ã‚·ãƒ§ãƒ³ãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆã®ã¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‰Šé™¤
        if args.cleanup:
            cleanup_training_dir("dataset")
            logger.info(
                "--cleanupã‚ªãƒ—ã‚·ãƒ§ãƒ³ã«ã‚ˆã‚Šã€datasetãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‰Šé™¤ã—ã¾ã—ãŸ"
            )

        logger.info("ã™ã¹ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ")
        return 0

    except Exception as e:
        logger.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return 1


if __name__ == "__main__":
    sys.exit(main())
